{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# import xgboost as xgb\n",
    "# from sklearn.svm import SVC\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "# from sklearn import pipeline\n",
    "# from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import SpatialDropout1D, LSTM\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai import *\n",
    "# from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bs=48\n",
    "# bs=16\n",
    "# #bs=192\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/data/home/manokuma/.local/bin/kaggle \n",
    "# import os\n",
    "# os.environ['KAGGLE_USERNAME'] = 'hackmano'\n",
    "# os.environ['KAGGLE_KEY'] = ''\n",
    "# !/data/home/manokuma/.local/bin/kaggle competitions download -c nlp-getting-started\n",
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Understanding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer,valid = train_test_split(train, random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 319034.80it/s]\n",
      "100%|██████████| 7613/7613 [00:00<00:00, 250716.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crushed.': 1, 'grandpa': 2, '@Change': 9, 'http://t.co/BDsgF1CfaX': 1, 'shift': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = train[\"text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            try:\n",
    "                a[word] = embeddings_index[word.lower()]\n",
    "                k += vocab[word]\n",
    "            except:\n",
    "                oov[word] = vocab[word]\n",
    "                i += vocab[word]\n",
    "                pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('@Sunnyclaribel. how many of @DPJHodges articles just attack labour or defend '\n",
      " 'conservatives? see for yourself: http://t.co/shAAIjO2ZC')\n"
     ]
    }
   ],
   "source": [
    "pprint(test.text.values[210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26234/26234 [00:00<00:00, 773831.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 64.20% of vocab\n",
      "Found embeddings for  94.00% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 1601),\n",
       " ('The', 586),\n",
       " ('A', 317),\n",
       " ('In', 161),\n",
       " ('It', 140),\n",
       " ('You', 117),\n",
       " ('California', 117),\n",
       " ('News', 112),\n",
       " ('S', 111),\n",
       " ('RT', 110),\n",
       " ('This', 110),\n",
       " ('To', 109),\n",
       " ('We', 108),\n",
       " ('New', 103),\n",
       " ('My', 98),\n",
       " ('If', 95),\n",
       " ('YouTube', 92),\n",
       " ('Hiroshima', 90),\n",
       " ('THE', 86),\n",
       " ('Is', 86)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'921' in word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = take(20, word_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neitz',\n",
      " 'lawter',\n",
      " 'haz',\n",
      " 'cytyc',\n",
      " 'kabayama',\n",
      " 'preclusive',\n",
      " 'jiazheng',\n",
      " 'adriatic',\n",
      " '176.5',\n",
      " 'spitze',\n",
      " 'merita',\n",
      " 'lightweights',\n",
      " 'jansons',\n",
      " 'dorcus',\n",
      " 'somatoform',\n",
      " 'metabolizing',\n",
      " '34-foot',\n",
      " '0.997',\n",
      " 'bauch',\n",
      " 'foot-long']\n"
     ]
    }
   ],
   "source": [
    "pprint(yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in '&/-?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, ' {0} '.format(punct))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 55294.39it/s]\n",
      "100%|██████████| 7613/7613 [00:00<00:00, 212592.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train[\"text\"] = train[\"text\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = train[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "with open('word_embeddings.pickle', 'rb') as g:\n",
    "    word_embeddings = pickle.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    "    #words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(word_embeddings[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if (type(v) != np.ndarray):\n",
    "        return np.zeros(300)\n",
    "    return v/np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "hell = sent2vec(\"MY first sentence in Python is not Hello world\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# word_embeddings = {}\n",
    "# f = open('glove.6B.300d.txt', encoding='utf8')\n",
    "# for line in tqdm(f):\n",
    "#     values = line.split()  \n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     word_embeddings[word] = coefs\n",
    "# f.close()\n",
    "# print('Found %s word vectors.' % len(word_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('word_embeddings.pickle', 'wb') as g:\n",
    "#     pickle.dump(word_embeddings,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_enc = np_utils.to_categorical(trainer.target.values)\n",
    "yvalid_enc = np_utils.to_categorical(valid.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6090/6090 [00:02<00:00, 2633.01it/s]\n",
      "100%|██████████| 1523/1523 [00:00<00:00, 2666.07it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in tqdm(trainer.text.values)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(valid.text.values)]\n",
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3263/3263 [00:00<00:00, 3569.47it/s]\n"
     ]
    }
   ],
   "source": [
    "test_glove = [sent2vec(x) for x in tqdm(test.text.values)]\n",
    "test_glove = np.array(test_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 300\n",
    "\n",
    "\n",
    "token.fit_on_texts(list(trainer.text.values) + list(valid.text.values))\n",
    "xtrain_seq = token.texts_to_sequences(trainer.text.values)\n",
    "xvalid_seq = token.texts_to_sequences(valid.text.values)\n",
    "test_seq = token.texts_to_sequences(test.text.values)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len, padding='post')\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len, padding='post')\n",
    "test_pad = sequence.pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrain_glove[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = take(20, word_index.items())\n",
    "yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21952/21952 [00:00<00:00, 467697.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = word_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()\n",
    "#train[\"text\"][1000]\n",
    "#word_embeddings.get(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try k-nearest neighbour on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use tf-idf on the text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=3, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=1, stop_words='english', strip_accents='unicode',\n",
       "                sublinear_tf=1, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=1, vocabulary=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv.fit(list(trainer.text.values) + list(valid.text.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_shape': (6090, 9229),\n",
      " 'data': array([0.33437892, 0.25069802, 0.06115396, ..., 0.31132424, 0.33215319,\n",
      "       0.26934142]),\n",
      " 'indices': array([8605, 8604, 7909, ..., 2824,  525,  524], dtype=int32),\n",
      " 'indptr': array([    0,    15,    21, ..., 76789, 76810, 76825], dtype=int32),\n",
      " 'maxprint': 50}\n"
     ]
    }
   ],
   "source": [
    "xtrain_tfv =  tfv.transform(trainer.text.values) \n",
    "xvalid_tfv = tfv.transform(valid.text.values)\n",
    "pprint(vars(xtrain_tfv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use Logistic Regression and linear svc on data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features used: 9193\n"
     ]
    }
   ],
   "source": [
    "#clf = KNeighborsClassifier(n_neighbors=5)\n",
    "#help(LogisticRegression)\n",
    "clf = LogisticRegression(C=2)\n",
    "clf.fit(xtrain_tfv, trainer.target)\n",
    "print(\"Number of features used:\", np.sum(clf.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set predictions: [0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#predictions = clf.predict_proba(xvalid_tfv)\n",
    "train_predictions =  clf.predict(xtrain_tfv)\n",
    "predictions =  clf.predict(xvalid_tfv)\n",
    "print(\"Test set predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training: 0.870 \n",
      "f1_score on test: 0.744 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on training: %0.3f \" % f1_score(trainer.target.values, train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.93\n",
      "Test set accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {:.2f}\".format(clf.score(xtrain_tfv, trainer.target)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(xvalid_tfv, valid.target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training: 0.916 \n"
     ]
    }
   ],
   "source": [
    "train_predictions = clf.predict(xtrain_tfv)\n",
    "print (\"f1_score on training: %0.3f \" % f1_score(trainer.target.values, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on test: 0.727 \n"
     ]
    }
   ],
   "source": [
    "#print (\"logloss: %0.3f \" % multiclass_logloss(valid.target.values, predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, predictions))\n",
    "#f1 score should be made as close to 1 as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1523"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.target.values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.38, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(LinearSVC)\n",
    "sv_clf = LinearSVC(C=0.38)\n",
    "sv_clf.fit(xtrain_tfv, trainer.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_predictions = sv_clf.predict(xvalid_tfv)\n",
    "sv_train_predictions = sv_clf.predict(xtrain_tfv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training: 0.896 \n",
      "f1_score on test: 0.741 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on training: %0.3f \" % f1_score(trainer.target.values, sv_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, sv_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use Naive bayes on the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = MultinomialNB(alpha=1)\n",
    "nb_clf.fit(xtrain_tfv, trainer.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predictions = nb_clf.predict(xvalid_tfv)\n",
    "nb_train_predictions = nb_clf.predict(xtrain_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training: 0.816 \n",
      "f1_score on test: 0.726 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on training: %0.3f \" % f1_score(trainer.target.values, nb_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, nb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using Count vectorizer instead of td-idf below<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctv.fit(list(trainer.text.values) + list(valid.text.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_ctv = ctv.transform(trainer.text.values)\n",
    "xvalid_ctv = ctv.transform(valid.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features used: 108907\n"
     ]
    }
   ],
   "source": [
    "clf_ctv = LogisticRegression(C=0.2)\n",
    "clf_ctv.fit(xtrain_ctv, trainer.target)\n",
    "print(\"Number of features used:\", np.sum(clf_ctv.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv_train_predictions = clf_ctv.predict(xtrain_ctv)\n",
    "ctv_predictions = clf_ctv.predict(xvalid_ctv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.719 \n",
      "f1_score on test: 0.619 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, ctv_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, ctv_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=75, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth = 75)\n",
    "dt_clf.fit(xtrain_tfv, trainer.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_predictions = dt_clf.predict(xtrain_tfv)\n",
    "dt_predictions = dt_clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.868 \n",
      "f1_score on test: 0.650 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, dt_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(RandomForestClassifier)\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train_predictions = forest.predict(xtrain_tfv)\n",
    "rfc_predictions = forest.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.983 \n",
      "f1_score on test: 0.700 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, rfc_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, rfc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Classifier|Data Transformer|Parameters|f1_Score_training|f1_Score_test|\n",
    "| --- | --- | --- | --- | --- |\n",
    "|LogisticRegression|tf-idf|C=2|0.870|0.744 \n",
    "|LinearSVC|tf-idf|C=0.38|0.896|0.741| \n",
    "|LinearSVC|tf-idf|C=0.1|0.836|0.734| \n",
    "|MultinomialNB|tf-idf|default(alpha=1)|0.816|0.726|\n",
    "|LogisticRegression|countVectorizer|liblinear/l1|0.857|0.738|\n",
    "|DecisionTreeClassifier|tf-idf|Default|0.983|0.656|\n",
    "|RandomForestClassifier|tf-idf|Default|0.983|0.700|\n",
    "|GradientBoostingClassifier|tf-idf|lr = 1|0.87|0.700|\n",
    "|xgboost|tf-idf|lr=0.3,nestimators=200,max_depth=5|0.856|0.718|\n",
    "|SVC|tf-idf|default|0.942|0.739|\n",
    "|MLP|tf-idf|max_iter=30,lbfgs,random_state=42|0.869|0.736|\n",
    "|XGB|glove|default|-|0.76|\n",
    "|LogisticRegression|glove|default|0.769|0.771|\n",
    "|MLP|glove|max_iter=50,adam|0.853|0.775|\n",
    "|Dense|glove,standardScaler|adam|0.85|0.805|\n",
    "|LSTM|glove,tokenize_padding|adam|0.9397|0.829|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_clf = GradientBoostingClassifier(learning_rate = 1)\n",
    "gbc_clf.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_train_predictions = gbc_clf.predict(xtrain_tfv)\n",
    "gbc_predictions = gbc_clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.877 \n",
      "f1_score on test: 0.690 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, gbc_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, gbc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(xgb)\n",
    "xgb_clf = xgb.XGBClassifier(max_depth = 5, n_estimators = 200, learning_rate = 0.1, nthread = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
       "              nthread=8, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_predictions = xgb_clf.predict(xtrain_tfv)\n",
    "xgb_predictions = xgb_clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.774 \n",
      "f1_score on test: 0.701 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, xgb_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(SVC)\n",
    "svc_clf = SVC()\n",
    "svc_clf.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_train_predictions = svc_clf.predict(xtrain_tfv)\n",
    "svc_predictions = svc_clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.942 \n",
      "f1_score on test: 0.739 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, svc_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, svc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scorer = metrics.make_scorer(f1_score, greater_is_better=True, needs_proba=False)\n",
    "svd = TruncatedSVD()\n",
    "scl = preprocessing.StandardScaler()\n",
    "lr_model = LogisticRegression()\n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                        ('scl', scl),\n",
    "                        ('lr', lr_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120,180],\n",
    "             'lr__C':[0.1, 1.0, 10],\n",
    "             'lr__penalty': ['l1','l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:   14.7s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   17.0s finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:825: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:0.689\n",
      "Best parameters set:\n",
      "\tlr__C: 10\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "model = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = f1_scorer, verbose = 10,n_jobs=-1,iid=True,\n",
    "                    refit = True, cv=2)\n",
    "model.fit(xtrain_tfv, trainer.target.values)\n",
    "print(\"Best score:%0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Applying Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=30,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='lbfgs',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(MLPClassifier)\n",
    "mlp = MLPClassifier(max_iter=30, solver='lbfgs')\n",
    "mlp.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_train_predictions = mlp.predict(xtrain_tfv)\n",
    "mlp_predictions = mlp.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.918 \n",
      "f1_score on test: 0.714 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, mlp_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, mlp_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Glove for Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-750b0dbed9c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_clf_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxgb_clf_glove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxgb_glove_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_clf_glove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "xgb_clf_glove = xgb.XGBClassifier(nthread=10, silent = False, max_depth=10, n_estimators=200, learning_rate=0.1)\n",
    "xgb_clf_glove.fit(xtrain_glove, trainer.target.values)\n",
    "xgb_glove_predictions = xgb_clf_glove.predict(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on test: 0.769 \n"
     ]
    }
   ],
   "source": [
    "#print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, xgb_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, xgb_glove_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#help(MLPClassifier)\n",
    "mlp_glove = MLPClassifier(max_iter=70, solver='adam')\n",
    "mlp_glove.fit(xtrain_glove, trainer.target.values)\n",
    "#lr_clf_glove = LinearSVC(C=0.38)\n",
    "#lr_clf_glove.fit(xtrain_glove, trainer.target.values)\n",
    "mlp_glove_predictions = mlp_glove.predict(xvalid_glove)\n",
    "mlp_glove_train_predictions = mlp_glove.predict(xtrain_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "key, val = next(iter(word_embeddings.items()))\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.890 \n",
      "f1_score on test: 0.759 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, mlp_glove_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, mlp_glove_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/5\n",
      "6090/6090 [==============================] - 5s 843us/step - loss: 0.6200 - f1: 0.7373 - val_loss: 0.4482 - val_f1: 0.8009\n",
      "Epoch 2/5\n",
      "6090/6090 [==============================] - 1s 100us/step - loss: 0.4564 - f1: 0.8056 - val_loss: 0.4242 - val_f1: 0.8035\n",
      "Epoch 3/5\n",
      "6090/6090 [==============================] - 1s 98us/step - loss: 0.3972 - f1: 0.8272 - val_loss: 0.4372 - val_f1: 0.8050\n",
      "Epoch 4/5\n",
      "6090/6090 [==============================] - 1s 100us/step - loss: 0.3687 - f1: 0.8455 - val_loss: 0.4445 - val_f1: 0.8014\n",
      "Epoch 5/5\n",
      "6090/6090 [==============================] - 1s 99us/step - loss: 0.3409 - f1: 0.8566 - val_loss: 0.4598 - val_f1: 0.7963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f605691c278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))\n",
    "#help(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_6_input to have shape (70,) but got array with shape (300,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2ac056fe9c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_glove_scl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_6_input to have shape (70,) but got array with shape (300,)"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(xvalid_glove_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 1, 1, ..., 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "predictions = predictions[:,0]\n",
    "pprint(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on test: 0.222 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=[f1])\n",
    "earlystop = EarlyStopping(monitor='val_f1', min_delta=0, patience=12, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 9s 1ms/step - loss: 0.6871 - f1: 0.5228 - val_loss: 0.6824 - val_f1: 0.5739\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 4s 735us/step - loss: 0.6837 - f1: 0.5693 - val_loss: 0.6822 - val_f1: 0.5739\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 5s 740us/step - loss: 0.6836 - f1: 0.5692 - val_loss: 0.6822 - val_f1: 0.5739\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 4s 737us/step - loss: 0.6839 - f1: 0.5699 - val_loss: 0.6825 - val_f1: 0.5739\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 5s 739us/step - loss: 0.6842 - f1: 0.5696 - val_loss: 0.6822 - val_f1: 0.5739\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 4s 739us/step - loss: 0.6844 - f1: 0.5696 - val_loss: 0.6826 - val_f1: 0.5739\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 4s 739us/step - loss: 0.6837 - f1: 0.5698 - val_loss: 0.6823 - val_f1: 0.5739\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 4s 735us/step - loss: 0.6835 - f1: 0.5692 - val_loss: 0.6822 - val_f1: 0.5739\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 5s 746us/step - loss: 0.6835 - f1: 0.5695 - val_loss: 0.6823 - val_f1: 0.5739\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 4s 735us/step - loss: 0.6836 - f1: 0.5697 - val_loss: 0.6823 - val_f1: 0.5739\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 5s 741us/step - loss: 0.6838 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5739\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 4s 735us/step - loss: 0.6844 - f1: 0.5694 - val_loss: 0.6824 - val_f1: 0.5739\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 4s 738us/step - loss: 0.6834 - f1: 0.5695 - val_loss: 0.6822 - val_f1: 0.5739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8eb99630f0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6897 - f1: 0.5679 - val_loss: 0.7693 - val_f1: 0.4260\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6906 - f1: 0.5634 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6859 - f1: 0.5691 - val_loss: 0.6835 - val_f1: 0.5740\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6844 - f1: 0.5725 - val_loss: 0.6841 - val_f1: 0.5740\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6847 - f1: 0.5708 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6861 - f1: 0.5689 - val_loss: 0.6835 - val_f1: 0.5740\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6838 - f1: 0.5724 - val_loss: 0.6834 - val_f1: 0.5740\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6842 - f1: 0.5706 - val_loss: 0.6825 - val_f1: 0.5740\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6839 - f1: 0.5706 - val_loss: 0.6827 - val_f1: 0.5740\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5724 - val_loss: 0.6826 - val_f1: 0.5740\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6839 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6837 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6837 - f1: 0.5671 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6837 - f1: 0.5688 - val_loss: 0.6824 - val_f1: 0.5740\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 28s 5ms/step - loss: 0.6836 - f1: 0.5688 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 27s 5ms/step - loss: 0.6836 - f1: 0.5680 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6838 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 28s 5ms/step - loss: 0.6836 - f1: 0.5706 - val_loss: 0.6824 - val_f1: 0.5740\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 27s 5ms/step - loss: 0.6836 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 21/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6837 - f1: 0.5688 - val_loss: 0.6824 - val_f1: 0.5740\n",
      "Epoch 22/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6837 - f1: 0.5715 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 23/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5715 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 24/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5688 - val_loss: 0.6824 - val_f1: 0.5740\n",
      "Epoch 25/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5697 - val_loss: 0.6824 - val_f1: 0.5740\n",
      "Epoch 26/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6838 - f1: 0.5680 - val_loss: 0.6824 - val_f1: 0.5740\n",
      "Epoch 27/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5688 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 28/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 29/100\n",
      "6090/6090 [==============================] - 27s 5ms/step - loss: 0.6840 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 30/100\n",
      "6090/6090 [==============================] - 27s 5ms/step - loss: 0.6840 - f1: 0.5680 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 31/100\n",
      "6090/6090 [==============================] - 28s 5ms/step - loss: 0.6837 - f1: 0.5680 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 32/100\n",
      "6090/6090 [==============================] - 27s 5ms/step - loss: 0.6836 - f1: 0.5680 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 33/100\n",
      "6090/6090 [==============================] - 28s 5ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 34/100\n",
      "6090/6090 [==============================] - 28s 5ms/step - loss: 0.6836 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 35/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 36/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 37/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6839 - f1: 0.5680 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 38/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 39/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 40/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 41/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6837 - f1: 0.5662 - val_loss: 0.6825 - val_f1: 0.5740\n",
      "Epoch 42/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6837 - f1: 0.5688 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 43/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 44/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5715 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 45/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6837 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 46/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 47/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5688 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 48/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5724 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 49/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 50/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 51/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 52/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6837 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 53/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 54/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6836 - f1: 0.5680 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 55/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5715 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 56/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6836 - f1: 0.5678 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 57/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6837 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 58/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 59/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6836 - f1: 0.5706 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 60/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6836 - f1: 0.5715 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 62/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 63/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 64/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 65/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6836 - f1: 0.5688 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 66/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5715 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 67/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6836 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 68/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 69/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 70/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 71/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 72/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 73/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 74/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 75/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 76/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 77/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 78/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 79/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 80/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 81/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6836 - f1: 0.5653 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 82/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 83/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 84/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5713 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 85/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6836 - f1: 0.5671 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 86/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6836 - f1: 0.5697 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 87/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6823 - val_f1: 0.5740\n",
      "Epoch 88/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 89/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 90/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 91/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 92/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 93/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5715 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 94/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5715 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 95/100\n",
      "6090/6090 [==============================] - 27s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 96/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5706 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 97/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 98/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5697 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 99/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5688 - val_loss: 0.6822 - val_f1: 0.5740\n",
      "Epoch 100/100\n",
      "6090/6090 [==============================] - 26s 4ms/step - loss: 0.6835 - f1: 0.5680 - val_loss: 0.6822 - val_f1: 0.5740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8eb97e75c0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = model.predict(xvalid_pad)\n",
    "predictions = model.predict(test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " #with open('predictions.pickle', 'wb') as g:\n",
    "     #pickle.dump(predictions,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "with open('predictions.pickle', 'rb') as g:\n",
    "    predictions = pickle.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 1, 1, ..., 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "predictions = predictions[:,0]\n",
    "pprint(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56965035, 0.43034968],\n",
       "       [0.56965035, 0.43034968],\n",
       "       [0.56965035, 0.43034968],\n",
       "       ...,\n",
       "       [0.56965035, 0.43034968],\n",
       "       [0.56965035, 0.43034968],\n",
       "       [0.56965035, 0.43034968]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution = test.iloc[:,0:1]\n",
    "#predictions = np.array(map(str, predictions))\n",
    "#predictions = np.insert(predictions, 0, \"target\", axis=0)\n",
    "#pprint(predictions[:])\n",
    "final_data = {'id' : test.id,'target':predictions}\n",
    "submission = pd.DataFrame(data=final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#help(MLPClassifier)\n",
    "lr_glove = LogisticRegression()\n",
    "lr_glove.fit(xtrain_glove, trainer.target.values)\n",
    "#lr_clf_glove = LinearSVC(C=0.38)\n",
    "#lr_clf_glove.fit(xtrain_glove, trainer.target.values)\n",
    "lr_glove_predictions = lr_glove.predict(xvalid_glove)\n",
    "lr_glove_train_predictions = lr_glove.predict(xtrain_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.760 \n",
      "f1_score on test: 0.765 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, lr_glove_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, lr_glove_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on test: 0.598 \n"
     ]
    }
   ],
   "source": [
    "#print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, lr_glove_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 1, 1, ..., 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_glove.predict(test_glove)\n",
    "pprint(predictions)\n",
    "#final_data = {'id' : test.id,'target':predictions}\n",
    "#submission = pd.DataFrame(data=final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = {'id' : test.id,'target':predictions}\n",
    "submission = pd.DataFrame(data=final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_lr_glove.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
