{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# import xgboost as xgb\n",
    "# from sklearn.svm import SVC\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "# from sklearn import pipeline\n",
    "# from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai import *\n",
    "# from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bs=48\n",
    "# bs=16\n",
    "# #bs=192\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/data/home/manokuma/.local/bin/kaggle \n",
    "# import os\n",
    "# os.environ['KAGGLE_USERNAME'] = 'hackmano'\n",
    "# os.environ['KAGGLE_KEY'] = ''\n",
    "# !/data/home/manokuma/.local/bin/kaggle competitions download -c nlp-getting-started\n",
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Understanding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer,valid = train_test_split(train,\n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_data': BlockManager\n",
      "Items: Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
      "Axis 1: RangeIndex(start=0, stop=7613, step=1)\n",
      "IntBlock: slice(0, 8, 4), 2 x 7613, dtype: int64\n",
      "ObjectBlock: slice(1, 4, 1), 3 x 7613, dtype: object,\n",
      " '_is_copy': None,\n",
      " '_item_cache': {}}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(word_embeddings[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if (type(v) != np.ndarray):\n",
    "        return np.zeros(300)\n",
    "    return v/np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:38, 10483.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.300d.txt', encoding='utf8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()  \n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/6090 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 178/6090 [00:00<00:03, 1777.09it/s]\u001b[A\n",
      "  9%|▊         | 528/6090 [00:00<00:02, 2084.35it/s]\u001b[A\n",
      " 14%|█▍        | 872/6090 [00:00<00:02, 2363.69it/s]\u001b[A\n",
      " 20%|██        | 1222/6090 [00:00<00:01, 2616.89it/s]\u001b[A\n",
      " 26%|██▌       | 1560/6090 [00:00<00:01, 2806.02it/s]\u001b[A\n",
      " 31%|███▏      | 1910/6090 [00:00<00:01, 2982.62it/s]\u001b[A\n",
      " 37%|███▋      | 2259/6090 [00:00<00:01, 3118.15it/s]\u001b[A\n",
      " 43%|████▎     | 2618/6090 [00:00<00:01, 3244.19it/s]\u001b[A\n",
      " 49%|████▉     | 2969/6090 [00:00<00:00, 3318.18it/s]\u001b[A\n",
      " 54%|█████▍    | 3319/6090 [00:01<00:00, 3369.06it/s]\u001b[A\n",
      " 60%|██████    | 3673/6090 [00:01<00:00, 3418.49it/s]\u001b[A\n",
      " 66%|██████▌   | 4030/6090 [00:01<00:00, 3459.62it/s]\u001b[A\n",
      " 72%|███████▏  | 4380/6090 [00:01<00:00, 3467.83it/s]\u001b[A\n",
      " 78%|███████▊  | 4737/6090 [00:01<00:00, 3496.01it/s]\u001b[A\n",
      " 84%|████████▎ | 5088/6090 [00:01<00:00, 3443.86it/s]\u001b[A\n",
      " 89%|████████▉ | 5433/6090 [00:01<00:00, 3411.17it/s]\u001b[A\n",
      "100%|██████████| 6090/6090 [00:01<00:00, 3363.42it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/1523 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|██▎       | 351/1523 [00:00<00:00, 3509.15it/s]\u001b[A\n",
      " 45%|████▍     | 680/1523 [00:00<00:00, 3439.95it/s]\u001b[A\n",
      " 68%|██████▊   | 1035/1523 [00:00<00:00, 3471.00it/s]\u001b[A\n",
      "100%|██████████| 1523/1523 [00:00<00:00, 3472.58it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in tqdm(trainer.text.values)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(valid.text.values)]\n",
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#OVOFest Drake straight body bagging Meek on that OVO stage. #ZIPHIMUP!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train.head()\n",
    "train[\"text\"][1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try k-nearest neighbour on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use tf-idf on the text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=3, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=1, stop_words='english', strip_accents='unicode',\n",
       "                sublinear_tf=1, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=1, vocabulary=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv.fit(list(trainer.text.values) + list(valid.text.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_shape': (6090, 9229),\n",
      " 'data': array([0.33437892, 0.25069802, 0.06115396, ..., 0.31132424, 0.33215319,\n",
      "       0.26934142]),\n",
      " 'indices': array([8605, 8604, 7909, ..., 2824,  525,  524], dtype=int32),\n",
      " 'indptr': array([    0,    15,    21, ..., 76789, 76810, 76825], dtype=int32),\n",
      " 'maxprint': 50}\n"
     ]
    }
   ],
   "source": [
    "xtrain_tfv =  tfv.transform(trainer.text.values) \n",
    "xvalid_tfv = tfv.transform(valid.text.values)\n",
    "pprint(vars(xtrain_tfv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use Logistic Regression and linear svc on data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features used: 9193\n"
     ]
    }
   ],
   "source": [
    "#clf = KNeighborsClassifier(n_neighbors=5)\n",
    "#help(LogisticRegression)\n",
    "clf = LogisticRegression(C=2)\n",
    "clf.fit(xtrain_tfv, trainer.target)\n",
    "print(\"Number of features used:\", np.sum(clf.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set predictions: [0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#predictions = clf.predict_proba(xvalid_tfv)\n",
    "train_predictions =  clf.predict(xtrain_tfv)\n",
    "predictions =  clf.predict(xvalid_tfv)\n",
    "print(\"Test set predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training: 0.870 \n",
      "f1_score on test: 0.744 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on training: %0.3f \" % f1_score(trainer.target.values, train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.93\n",
      "Test set accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {:.2f}\".format(clf.score(xtrain_tfv, trainer.target)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(xvalid_tfv, valid.target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training: 0.916 \n"
     ]
    }
   ],
   "source": [
    "train_predictions = clf.predict(xtrain_tfv)\n",
    "print (\"f1_score on training: %0.3f \" % f1_score(trainer.target.values, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on test: 0.727 \n"
     ]
    }
   ],
   "source": [
    "#print (\"logloss: %0.3f \" % multiclass_logloss(valid.target.values, predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, predictions))\n",
    "#f1 score should be made as close to 1 as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1523"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.target.values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.38, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(LinearSVC)\n",
    "sv_clf = LinearSVC(C=0.38)\n",
    "sv_clf.fit(xtrain_tfv, trainer.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_predictions = sv_clf.predict(xvalid_tfv)\n",
    "sv_train_predictions = sv_clf.predict(xtrain_tfv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training: 0.896 \n",
      "f1_score on test: 0.741 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on training: %0.3f \" % f1_score(trainer.target.values, sv_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, sv_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use Naive bayes on the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = MultinomialNB(alpha=1)\n",
    "nb_clf.fit(xtrain_tfv, trainer.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predictions = nb_clf.predict(xvalid_tfv)\n",
    "nb_train_predictions = nb_clf.predict(xtrain_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training: 0.816 \n",
      "f1_score on test: 0.726 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on training: %0.3f \" % f1_score(trainer.target.values, nb_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, nb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using Count vectorizer instead of td-idf below<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctv.fit(list(trainer.text.values) + list(valid.text.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_ctv = ctv.transform(trainer.text.values)\n",
    "xvalid_ctv = ctv.transform(valid.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features used: 108907\n"
     ]
    }
   ],
   "source": [
    "clf_ctv = LogisticRegression(C=0.2)\n",
    "clf_ctv.fit(xtrain_ctv, trainer.target)\n",
    "print(\"Number of features used:\", np.sum(clf_ctv.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv_train_predictions = clf_ctv.predict(xtrain_ctv)\n",
    "ctv_predictions = clf_ctv.predict(xvalid_ctv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.719 \n",
      "f1_score on test: 0.619 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, ctv_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, ctv_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=75, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth = 75)\n",
    "dt_clf.fit(xtrain_tfv, trainer.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_predictions = dt_clf.predict(xtrain_tfv)\n",
    "dt_predictions = dt_clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.868 \n",
      "f1_score on test: 0.650 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, dt_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(RandomForestClassifier)\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train_predictions = forest.predict(xtrain_tfv)\n",
    "rfc_predictions = forest.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.983 \n",
      "f1_score on test: 0.700 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, rfc_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, rfc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Classifier|Data Transformer|Parameters|f1_Score_training|f1_Score_test|\n",
    "| --- | --- | --- | --- | --- |\n",
    "|LogisticRegression|tf-idf|C=2|0.870|0.744 \n",
    "|LinearSVC|tf-idf|C=0.38|0.896|0.741| \n",
    "|LinearSVC|tf-idf|C=0.1|0.836|0.734| \n",
    "|MultinomialNB|tf-idf|default(alpha=1)|0.816|0.726|\n",
    "|LogisticRegression|countVectorizer|liblinear/l1|0.857|0.738|\n",
    "|DecisionTreeClassifier|tf-idf|Default|0.983|0.656|\n",
    "|RandomForestClassifier|tf-idf|Default|0.983|0.700|\n",
    "|GradientBoostingClassifier|tf-idf|lr = 1|0.87|0.700|\n",
    "|xgboost|tf-idf|lr=0.3,nestimators=200,max_depth=5|0.856|0.718|\n",
    "|SVC|tf-idf|default|0.942|0.739|\n",
    "|MLP|tf-idf|max_iter=30,lbfgs,random_state=42|0.869|0.736|\n",
    "|XGB|glove|default|-|0.76|\n",
    "|LogisticRegression|glove|default|0.769|0.771|\n",
    "|MLP|glove|max_iter=50,adam|0.853|0.775|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_clf = GradientBoostingClassifier(learning_rate = 1)\n",
    "gbc_clf.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_train_predictions = gbc_clf.predict(xtrain_tfv)\n",
    "gbc_predictions = gbc_clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.877 \n",
      "f1_score on test: 0.690 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, gbc_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, gbc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(xgb)\n",
    "xgb_clf = xgb.XGBClassifier(max_depth = 5, n_estimators = 200, learning_rate = 0.1, nthread = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=None, n_estimators=200, n_jobs=1,\n",
       "              nthread=8, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_predictions = xgb_clf.predict(xtrain_tfv)\n",
    "xgb_predictions = xgb_clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.774 \n",
      "f1_score on test: 0.701 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, xgb_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(SVC)\n",
    "svc_clf = SVC()\n",
    "svc_clf.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_train_predictions = svc_clf.predict(xtrain_tfv)\n",
    "svc_predictions = svc_clf.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.942 \n",
      "f1_score on test: 0.739 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, svc_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, svc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scorer = metrics.make_scorer(f1_score, greater_is_better=True, needs_proba=False)\n",
    "svd = TruncatedSVD()\n",
    "scl = preprocessing.StandardScaler()\n",
    "lr_model = LogisticRegression()\n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                        ('scl', scl),\n",
    "                        ('lr', lr_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120,180],\n",
    "             'lr__C':[0.1, 1.0, 10],\n",
    "             'lr__penalty': ['l1','l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:   14.7s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   17.0s finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:825: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:0.689\n",
      "Best parameters set:\n",
      "\tlr__C: 10\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "model = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = f1_scorer, verbose = 10,n_jobs=-1,iid=True,\n",
    "                    refit = True, cv=2)\n",
    "model.fit(xtrain_tfv, trainer.target.values)\n",
    "print(\"Best score:%0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Applying Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=30,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='lbfgs',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(MLPClassifier)\n",
    "mlp = MLPClassifier(max_iter=30, solver='lbfgs')\n",
    "mlp.fit(xtrain_tfv, trainer.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_train_predictions = mlp.predict(xtrain_tfv)\n",
    "mlp_predictions = mlp.predict(xvalid_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.918 \n",
      "f1_score on test: 0.714 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, mlp_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, mlp_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Glove for Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_glove = xgb.XGBClassifier(nthread=10, silent = False, max_depth=10, n_estimators=200, learning_rate=0.1)\n",
    "xgb_clf_glove.fit(xtrain_glove, trainer.target.values)\n",
    "xgb_glove_predictions = xgb_clf_glove.predict(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on test: 0.769 \n"
     ]
    }
   ],
   "source": [
    "#print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, xgb_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, xgb_glove_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#help(MLPClassifier)\n",
    "mlp_glove = MLPClassifier(max_iter=70, solver='adam')\n",
    "mlp_glove.fit(xtrain_glove, trainer.target.values)\n",
    "#lr_clf_glove = LinearSVC(C=0.38)\n",
    "#lr_clf_glove.fit(xtrain_glove, trainer.target.values)\n",
    "mlp_glove_predictions = mlp_glove.predict(xvalid_glove)\n",
    "mlp_glove_train_predictions = mlp_glove.predict(xtrain_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "key, val = next(iter(word_embeddings.items()))\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on train: 0.890 \n",
      "f1_score on test: 0.759 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on train: %0.3f \" % f1_score(trainer.target.values, mlp_glove_train_predictions))\n",
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, mlp_glove_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)\n",
    "ytrain_enc = np_utils.to_categorical(trainer.target.values)\n",
    "yvalid_enc = np_utils.to_categorical(valid.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/5\n",
      "6090/6090 [==============================] - 5s 776us/step - loss: 0.6490 - val_loss: 0.4715\n",
      "Epoch 2/5\n",
      "6090/6090 [==============================] - 1s 89us/step - loss: 0.4618 - val_loss: 0.4403\n",
      "Epoch 3/5\n",
      "6090/6090 [==============================] - 1s 89us/step - loss: 0.3982 - val_loss: 0.4523\n",
      "Epoch 4/5\n",
      "6090/6090 [==============================] - 1s 88us/step - loss: 0.3768 - val_loss: 0.4777\n",
      "Epoch 5/5\n",
      "6090/6090 [==============================] - 1s 89us/step - loss: 0.3392 - val_loss: 0.4658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f4c467a3320>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))\n",
    "#help(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(xvalid_glove_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 1, 1, ..., 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "predictions = predictions[:,0]\n",
    "pprint(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on test: 0.222 \n"
     ]
    }
   ],
   "source": [
    "print (\"f1_score on test: %0.3f \" % f1_score(valid.target.values, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
